# -*- coding: utf-8 -*-
"""Sentiment_Analysis_Sequence_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/abhiraman/data_mining/blob/main/Sentiment_Analysis_Sequence_Classification.ipynb

Sentiment Analysis is a Sequence Classification Problem. Here The labels are Positive & Negative.

Data Set : https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv

https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec
"""

import nltk
from nltk.corpus import stopwords
import pandas as pd
import regex as re
from sklearn.model_selection import train_test_split
import plotly.express as px
from nltk.stem.wordnet import WordNetLemmatizer
from pprint import pprint
from collections  import Counter
import torch
import torch.nn as nn
from torch.utils.data import Dataset,DataLoader
from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence
import torch.optim as optim
from sklearn.metrics import accuracy_score

# Commented out IPython magic to ensure Python compatibility.
nltk.download(["stopwords","wordnet"])
# %cd /root/nltk_data/corpora/stopwords
stop_Words = stopwords.words("english")

from google.colab import drive
drive.mount('/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /gdrive/MyDrive/IMDB_Senti_Analysis
!ls

isCuda = torch.cuda.is_available()
if isCuda:
  Mydevice = torch.device("cuda")
else:
  Mydevice = torch.device("cpu")

print(Mydevice)

main_df = pd.read_csv('IMDB Dataset.csv')

main_df.head()

"""# Split Data"""

## Converting Positive ->1 and negative -> 0
main_df.sentiment[main_df.sentiment=="positive"]=1
main_df.sentiment[main_df.sentiment=="negative"]=0

main_df.head()

main_df["review"][1]

fig = px.bar(main_df,x=["Positive Review","Negative Review"],y = main_df["sentiment"].value_counts(),)
fig.show()

X,Y = main_df["review"].values,main_df["sentiment"].values ## Converting pd.series -> np array
X_train,X_test,Y_train,Y_split = train_test_split(X,Y,test_size=0.9,stratify=Y)
print(X_train.shape,X_test.shape)

"""
---
# Cleaning Data - Tokenization
***

"""

def _string_cleanUp(arrOf_strs):
  count=0
  listOf_Strs = []
  for e_str in arrOf_strs:
    e_str = e_str.lower()   ## Loawer Casing the entire string
    e_str = re.sub(r'<[^>]*>','',e_str) ## Removing HTML Tags
    e_str = re.sub(r"[^\w\s]", ' ', e_str) ## Remove Special Characters 
    e_str = re.sub(r"\d", '', e_str) ## Remove Numbers from string
    count+=1
    listOf_Strs.append(e_str)
  return listOf_Strs

Cleaned_Sentences = _string_cleanUp(X_train)
for e_line in Cleaned_Sentences[0:5]:
  print(e_line)
  print("\n")

def _token_StringList(StrList,lemObj):
  
  wordList,spl_strs  = [],["<sos>","<eos>","<pad>"]
  for eLine in StrList:
    eLine = eLine.split(" ")
    for eWord in eLine:
      if eWord in stop_Words:continue ## Skipping stop words
      else:
        if  eWord == '':continue
        eWord = lemObj.lemmatize(eWord)
        wordList.append(eWord)
  return wordList

wl = WordNetLemmatizer()
wordToken = _token_StringList(Cleaned_Sentences,wl)

#wordToken = {ind:word for ind,word in enumerate(spl_strs+wordList)}

wordDict = Counter(wordToken)
print(wordDict)

def _return_most_recurringVocab(worDict):
  spl_strs = ["<pad>"]
  vList = [x[0] for x in sorted(worDict.items(),key=lambda x:x[1],reverse=True)[:1000]]
  return {word:ind for ind,word in enumerate(spl_strs+vList)}

"""# Train & Test Data(Indexed Vocab)"""

trainVocab = _return_most_recurringVocab(wordDict)
print(trainVocab)

# ## Similar activity for Test Data ##
# test_Cleaned_Sentences =  _string_cleanUp(X_test)
# testWordToken = _token_StringList(test_Cleaned_Sentences,wl)

# testVocab = _return_most_recurringVocab(Counter(testWordToken))
# print(testVocab)

"""# Custom Data Loader """

## Create a custom dataset loader ## 
class _reviews_loader(Dataset):
  def __init__(self,csv_file_name = None):
    super().__init__()
    print(dir(_reviews_loader))
    self.d_frame = pd.read_csv(csv_file_name)
  
  def __len__(self):
    #d_frame = pd.read_csv(csv_file_name)
    return len(self.d_frame)
  
  def __getitem__(self,idx):
    returnDict = (self.d_frame["review"][idx],self.d_frame["sentiment"][idx])
    return returnDict
review_dataset = _reviews_loader(csv_file_name='IMDB Dataset.csv')

a = [1,2,3,4]
bb = torch.empty((4,),dtype= torch.int64,device=Mydevice)
for i in range(bb.size()[-1]):
  bb[i] = a[i]

print(bb.dtype)
print(bb.device)

class MyCollateClass():
  def __init__(self,vocabDict = None):
    self.vocabDict = vocabDict

  def _string_cleanUp(arrOf_strs):
    count=0
    listOf_Strs = []
    for e_str in arrOf_strs:
      e_str = e_str.lower()   ## Loawer Casing the entire string
      e_str = re.sub(r'<[^>]*>','',e_str) ## Removing HTML Tags
      e_str = re.sub(r"[^\w\s]", ' ', e_str) ## Remove Special Characters 
      e_str = re.sub(r"\d", '', e_str) ## Remove Numbers from string
      count+=1
      listOf_Strs.append(e_str)
    return listOf_Strs


  
  def _return_indexList(self,OneSentance):
    vocabIndexes = []
    for eWord in OneSentance.split(" "):
      if eWord in list(self.vocabDict.keys()):
        vocabIndexes.append(self.vocabDict[eWord])
    idx_Tensor = torch.LongTensor(vocabIndexes)
    return idx_Tensor
  
  def _stack_Sentance_info(self,max_sentence_len = None,batch_size=None,device='cpu'):
    tensorList,updatedTensorList,seqLengths = [],[],[]
    for ind,eLine in enumerate(self.cleanedList):
      retTensor = self._return_indexList(eLine)
      tensorList.append(retTensor)
    maxTensorSize = max(list((e_Tensor.size()[0] for e_Tensor in tensorList)))
    for e_tensor in tensorList:
      seqLengths.append(e_tensor.size()[0])
      if e_tensor.size()[0]<maxTensorSize:
        diff = maxTensorSize - e_tensor.size()[0]
        newTensor = torch.cat([e_tensor,torch.zeros(diff)])
        updatedTensorList.append(newTensor)
      else:updatedTensorList.append(e_tensor)
    finalTensor = torch.stack(updatedTensorList).type(torch.LongTensor).to(device)
    return finalTensor,seqLengths

  def PadCollate(self,batch):
    def _get_max_sentance_len(SentanceList):
      return max(list((len(esentance.split(' ')) for esentance in SentanceList)))
    def _convert_senti_to_int(SentList,device='cpu'):
      slist =[]
      for i in range(len(SentList)):
        if SentList[i]=="positive":slist.append(1)
        else:slist.append(0)
      sTensor = torch.LongTensor(slist)
      return sTensor
    batch_Dict = {}
    revList = list((eTuple[0] for eTuple in batch))
    sentiList = list((eTuple[1] for eTuple in batch))
    stacked_senti_tensor = _convert_senti_to_int(sentiList,device=Mydevice).to(Mydevice)
    self.cleanedList = _string_cleanUp(revList)
    maxLen_sentance = _get_max_sentance_len(self.cleanedList)
    stacked_vocab_tensor,seqLengths = self._stack_Sentance_info(maxLen_sentance,len(batch),device=Mydevice)
    batch_Dict = {"Vocab":stacked_vocab_tensor,"Senti":stacked_senti_tensor,"Seqlen":seqLengths}
    return batch_Dict


  def __call__(self,batch):
    return self.PadCollate(batch)

dataloader1 = DataLoader(review_dataset,batch_size = 1000,shuffle=True, num_workers=0,collate_fn=MyCollateClass(trainVocab))

## Test The data from data loader ##
for ind,data in enumerate(dataloader1):
  if ind>3:break
  print(data["Vocab"].device)
  print(data["Senti"])
  print(data["Senti"].device)
  print(data["Vocab"].shape)
  print(data["Senti"].shape)
  print("seq lenght",data["Seqlen"])
  print('*'*75)

"""MODEL
---
"""

class SentiClassify_Model(nn.Module):
  def __init__(self,vocabLen,dims,hidden_size,seqLengths,output_size=2):
    super().__init__()
    #output_size =  2
    self.hidden_size = hidden_size
    self.seqLengths = seqLengths
    self.embed = nn.Embedding(vocabLen,dims)
    self.lstm_cell = nn.LSTM(input_size=dims,hidden_size=hidden_size)
    self.lf = nn.Linear(max(seqLengths)*hidden_size,(max(seqLengths)*hidden_size)//2)
    self.lf1 = nn.Linear((max(seqLengths)*hidden_size)//2,64)
    self.lf2 = nn.Linear(64,output_size)
    #self.sf_max = nn.Softmax(dim=1)
    

  def forward(self,input,hidden,verbose=False):
    embeds = self.embed(input)
    packedSeq = pack_padded_sequence(embeds.permute(1,0,2),self.seqLengths,batch_first=True,enforce_sorted=False)
    output,hidden = self.lstm_cell(packedSeq,hidden)
    outputt, input_sizes = pad_packed_sequence(output, batch_first=True)
    reshaped_out = outputt.reshape(outputt.size()[0],outputt.size()[1]*outputt.size()[2])
    lin_output = self.lf(reshaped_out)
    lin_output1 = self.lf1(lin_output)
    lin_output2 = self.lf2(lin_output1)
    return lin_output2

    if verbose:
      print("input shape",input.shape)
      print('embed shape',embeds.shape)
      print("Rehaped output",reshaped_out.size())
      print("After Fully conn layer :",lin_output.size())

  def init_hiddenlayer(self,batch_size,device='cpu'):
    return (torch.zeros(1,batch_size,self.hidden_size,device=device),torch.zeros(1,batch_size,self.hidden_size,device=device))

def computeAccuracy(target,source):
  sf_max_obj = nn.Softmax(dim=1)
  sf_max = sf_max_obj(source)
  sf_max = torch.argmax(sf_max,dim=1)
  fintensor = torch.where(sf_max==1,1,0) ## 1-> positive ,0->Negative
  score = accuracy_score(target.tolist(),fintensor.tolist())
  return score

def _trainLoader(model=None,dataloader=None,vocabList = None,optimm =None,loss_func=None,epochs=1,device='cpu'):
  ## Hyperparameters ##
  dims = 10
  hidden_size = 20
  
  loss_per_epoch,train_accuracy,test_accuracy = 0,None,None
  ## Batch Optimization ##
  for i in range(epochs):
    cummLoss = 0
    for ind,data in enumerate(dataloader):
      wordInput,seqLengths,targets = data["Vocab"].permute(1,0),data["Seqlen"],data["Senti"]
      modObj = model(len(vocabList),dims,hidden_size,seqLengths).to(device)
      hidden = modObj.init_hiddenlayer(wordInput.size()[-1],device=device)
      source = modObj(wordInput,hidden)
      if optimm==None:
        optimm = optim.Adam(modObj.parameters(),lr=0.005)
      loss = lossFn(source,targets)
      loss.backward(retain_graph=True)
      optimm.zero_grad()
      optimm.step()
      cummLoss+=loss.item()*source.size()[0] ## Cumulative loss per batch

    train_accuracy = computeAccuracy(targets,source)
    loss_per_epoch = cummLoss/5000
    print("Loss per Epoch : {} , Training Accuracy : {}".format(loss_per_epoch,train_accuracy))

lossFn = nn.CrossEntropyLoss()
_trainLoader(model=SentiClassify_Model,dataloader=dataloader1,vocabList = trainVocab,loss_func=lossFn,epochs=10,device=Mydevice)