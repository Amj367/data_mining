{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Sequence_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiraman/data_mining/blob/master/Sentiment_Analysis_Sequence_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4IovauFIT4B"
      },
      "source": [
        "Sentiment Analysis is a Sequence Classification Problem. Here The labels are Positive & Negative.\n",
        "\n",
        "Data Set : https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv\n",
        "\n",
        "https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQkVUtFPIM7a"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import plotly.express as px\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from pprint import pprint\n",
        "from collections  import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fDQBYgBMieq",
        "outputId": "763058ee-5c02-405c-ef65-14c5ad6f703b"
      },
      "source": [
        "nltk.download([\"stopwords\",\"wordnet\"])\n",
        "%cd /root/nltk_data/corpora/stopwords\n",
        "stop_Words = stopwords.words(\"english\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "/root/nltk_data/corpora/stopwords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTuzoMXWOBKr",
        "outputId": "235b7d29-bed4-4d6b-9fd5-d2ea2a22be7c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i_dNlIlM9se",
        "outputId": "65b8d6e8-a049-4685-bde7-1358f146a4d7"
      },
      "source": [
        "%cd /gdrive/MyDrive/IMDB_Senti_Analysis\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/IMDB_Senti_Analysis\n",
            "'IMDB Dataset.csv'   IMDB_Words_Vocab.csv   model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvwppcW54UrZ"
      },
      "source": [
        "isCuda = torch.cuda.is_available()\n",
        "if isCuda:\n",
        "  Mydevice = torch.device(\"cuda\")\n",
        "else:\n",
        "  Mydevice = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3oigX99P57b"
      },
      "source": [
        "main_df = pd.read_csv('IMDB Dataset.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZfMt9XTSQPS3",
        "outputId": "b714dcf0-c76d-4eed-f98b-c755fae8560d"
      },
      "source": [
        "main_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKSe675ZawUz"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht5x75ngay4L"
      },
      "source": [
        "## Converting Positive ->1 and negative -> 0\n",
        "main_df.sentiment[main_df.sentiment==\"positive\"]=1\n",
        "main_df.sentiment[main_df.sentiment==\"negative\"]=0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BKhxAQsVfpg_",
        "outputId": "80a1ad28-5ff4-4fc4-8b88-fe66a489be13"
      },
      "source": [
        "main_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...         1\n",
              "1  A wonderful little production. <br /><br />The...         1\n",
              "2  I thought this was a wonderful way to spend ti...         1\n",
              "3  Basically there's a family where a little boy ...         0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "nTRJFcW-pOqp",
        "outputId": "3de95405-ab2e-490e-e0c6-d6a8b70f74b4"
      },
      "source": [
        "main_df[\"review\"][1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "L4noOYWFhS65",
        "outputId": "b8fb89f9-97b3-46ad-e997-86035a211737"
      },
      "source": [
        "fig = px.bar(main_df,x=[\"Positive Review\",\"Negative Review\"],y = main_df[\"sentiment\"].value_counts(),)\n",
        "fig.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9fd764f0-63b7-4908-8b21-8f11ed6aa7b7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9fd764f0-63b7-4908-8b21-8f11ed6aa7b7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9fd764f0-63b7-4908-8b21-8f11ed6aa7b7',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"x=%{x}<br>y=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"Positive Review\", \"Negative Review\"], \"xaxis\": \"x\", \"y\": [25000, 25000], \"yaxis\": \"y\"}],\n",
              "                        {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9fd764f0-63b7-4908-8b21-8f11ed6aa7b7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWRjeTcKf62P",
        "outputId": "88eadb49-009a-4b0e-d1b5-54b7d0ade3ef"
      },
      "source": [
        "X,Y = main_df[\"review\"].values,main_df[\"sentiment\"].values ## Converting pd.series -> np array\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.4,stratify=Y)\n",
        "print(X_train.shape,X_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000,) (20000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm10ENc2Vts2"
      },
      "source": [
        "\n",
        "---\n",
        "# Cleaning Data - Tokenization\n",
        "***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pzOgHNmM87Q",
        "outputId": "5b75462f-659e-4929-b81a-98e85f52a323"
      },
      "source": [
        "def _string_cleanUp(arrOf_strs):\n",
        "  count=0\n",
        "  listOf_Strs = []\n",
        "  for e_str in arrOf_strs:\n",
        "    e_str = e_str.lower()   ## Loawer Casing the entire string\n",
        "    e_str = re.sub(r'<[^>]*>','',e_str) ## Removing HTML Tags\n",
        "    e_str = re.sub(r\"[^\\w\\s]\", ' ', e_str) ## Remove Special Characters \n",
        "    e_str = re.sub(r\"\\d\", '', e_str) ## Remove Numbers from string\n",
        "    count+=1\n",
        "    listOf_Strs.append(e_str)\n",
        "  return listOf_Strs\n",
        "\n",
        "Cleaned_Sentences = _string_cleanUp(X_train)\n",
        "for e_line in Cleaned_Sentences[0:5]:\n",
        "  print(e_line)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "red sonja is a career step in the wrong direction for arnold schwarzenegger  having made a couple of sword  n  sorcery films  as conan  he had moved onto slightly more serious acting roles in films like the terminator and commando  only to make a mystifying return to the sword  n  sorcery genre for this  debacle  it s hard to figure out why he bothered  as this is weaker than both conan films in every conceivable department  allegedly  this was to have been the third conan film  but for one reason or another the emphasis was shifted onto the leading female character  the titular red head  leaving poor old arnold to play an incredibly dull supporting role  spare a thought  too  for director richard fleischer who had given the world classics like   leagues under the sea  fantastic voyage  the boston strangler and  rillington place  in this   his penultimate film   fleischer also has taken a gigantic career step backwards evil queen gedren  sandahl bergman  wants to rule the world  and she needs a priceless and powerful talisman to do so  she and her brutish army storm a keep populated by priestesses and steal the said talisman  massacring the helpless priestesses as they go  one of the dead priestesses has a sister named sonja  brigitte nielsen   a fiery red headed warrior  who upon hearing of her sister s death swears revenge upon the evil gedren  sonja rides across the land in search of gedren s lair  along the way she picks up travelling companions in the shape of a boy prince  tarn  ernie reyes jr  and his bodyguard falkon  paul smith   she also meets the muscular warrior conan   sorry  i mean kalidor       who offers to join her in her quest  initially sonja doesn t want the help of kalidor  arnold schwarzenegger   preferring instead to prove that she can confront and defeat her enemies alone  but eventually she warms to him and accepts his assistance red sonja is a staggeringly poor film  all the more so when one muses that it was made in  when the sword  n  sorcery genre was close to its end  it seems so simplistic and amateurish that one could easily mistake it for an early example of its kind  the performances are poor on the whole  ranging from bergman s embarrassingly ott villain to reyes  unbelievably irritating spoilt brat to schwarzenegger s wooden and unenthusiastic hero  nielsen is slightly better as the heroine   presumably full of enthusiasm at the thought of being in her first starring role   but she is let down very badly by the stupidity of clive exton and george macdonald fraser s script  the film is riddled with goofs  including a scene where schwarzenegger is seen in close up hacking down bad guys but in a long shot in the same sequence there isn t a corpse in sight  technically it is very inept too  with sub standard special effects and appallingly mechanical monsters  there are a few compensations  such as ennio morricone s enjoyable music  morricone spent a great deal of the  s providing good music for awful films  e g the island  treasure of the four crowns and hundra   another compensation is giuseppe rotunno s lensing of the locations   in fact  much of the time it s a hell of a lot more gratifying to look at the lovely scenery than the actors standing in the foreground  there were very few sword  n  sorcery films after red sonja  so in some ways it might go down in history as the film which destroyed its own genre \n",
            "\n",
            "\n",
            "look  if i were interested in a nancy drew book  what i would do is pick up a book and read it  i m not  ever since i can remember i read people trashing movies because it wasn t like the book  i m sorry   in the digital age we can no longer watch movies on flip books  however i m sure you can still find a few short silent films in book form  when lord of the rings came out  people complained  when the third one won an oscar    the book was better   when i watched to kill a mockingbird   the book was better   now a bunch of people are upset  yet again  because nancy drew wasn t like the book  i m not saying nancy drew is going to win any oscars   if anything it ll be one of those nickelodeon blimps or kids choice awards  i m saying give film a break  it s film  not paper  as a movie  i found nancy drew quite enjoyable   featuring cameos from bruce willis and adam goldberg  the hebrew hammer  and supporting roles featuring tate donovan  jimmy cooper on the o c   and rachel leigh cook  she s all that   this is the first time i ve seen emma roberts in a movie and  frankly  i enjoy her work more than most of julia and of eric s  her character stays consistent throughout the film and reacts well with conflict  a lighthearted movie in the spirit of harriet the spy is nice now and again i give it ten stars because i thoroughly enjoyed the movie  would love to see it again  and will probably buy it upon dvd release \n",
            "\n",
            "\n",
            "this can be one of the most enjoyable movies ever if you don t take it seriously  it is a bit dated and the effects are lame  but it is so enjoyable  there are giant crabs that attack a girl  oh  and the crabs sing japanese  it is amazingly bad  and the ending  which has been telegraphed throughout the entire film is hideously awesome  predictable  but seeing the final fight will leave you rolling in your seat  don t even give this film a chance and you will love it  susan george is fun to watch and yes  she does appear naked  her daughter isn t quite worth putting up with  but she does get attacked by giant crabs  they are the size of large cats  this is a   but i love it  as a movie  my god  but for entertainment  i give it a   did i mention there are giant crabs \n",
            "\n",
            "\n",
            "i rented this movie  after hearing chris gore saying something to the effect of  five stars   on that attack of the show show  well when i turned around the dvd and it showed the  stages of hell  well i had to buy it  just to see the spectacle of a mother yelling at her son to drop her other son into a flaming pit i wasn t expecting ecw or czw for an hour and eighteen minutes  but i was expecting at least a summarized version of what seemed to be the main highlight of this movie  well sadly there wasn t anything like that  the  stages of death part happens right from the beginning  and its pretty much downhill from there  nothing really happens in this documentary  it was pretty raw  bare and unbiased  not a bad thing  but there is a narrator in this one  you d expect him to have opinions on the subject of this documentary  but he doesn t  which would of been nice to have  a message or reason for this doc there was no real reason to have a narrator  there should of been just text explaining some of the less obvious scenes it doesn t really explain the lives of these wrestlers either  it shows a few moments of some dramatic scenes  which sound interesting  but the reality isn t as great as it sounds  for instance mom watching her son wrestle with light bulbs and tacks  for the first time  at a public park  instead of seeing her reaction to the wrestling  they show her reacting to the camera  instead of say a interview later on  or just actually witnessing her reactions legitamit document wise  this one ain t  the source material was flimsy to begin with  nothing truly profound or interesting really happens  no conclusion to a few of the more interesting stories  no real point or final thought to backyard wrestling  edited together badly  and its and its basically a cheap  failed rip off of beyond the mat wrestling wise  this is pretty boring  the better bumps are at the beginning  and slowly become less amazing and shocking  if you have seen japanese wrestling  indie wrestling  or even backyard wrestling dvds  than this wont shock and awe you  if you want wrestling don t make the same mistake i did and see this one  go get some czw ecw or xpw dvds instead the only thing i got out of this documentary was how stupid people can be  not for supporting self mutilation or doing dangerous stunts  but their reasoning for committing these acts  the backyarders seem stupid for wrestling  most of them are jobless  and probably have a few issues in their head  and wrestling is a type of therapy for them  than the supporters seem even more idiotic  mothers basically take the whole  if ya cant beat em join em  reasoning to cope with the fact that their sons are basically killing themselves  school authority figures support their students in their dangerous stunts because its an alternative to joining gangs and to a lesser extend doing drugs  which is kinda funny since that segment took place in a rural town  where like people live  miles from one another  people are stupid  thats what i extracted from this documentary if you want to see the reasoning and thoughts to someone brutalizing themselves in wrestling and basically what the back of this dvd promises  get unscarred  the life of nick mondo  its more amazing  and interesting than the backyard  and a lot more entertaining  oh and its actually good \n",
            "\n",
            "\n",
            "i really loved this movie and have spent several years trying to get it  it is just not available and it has not been on tv for many many years  i enjoyed it and the songs because it had something different to say and made you think how every person looks at something from different prespectives  also we often don t appreciate something we have till it is no longer there my  year old daughter just discoverd the music and is entranced with some of the songs  someday i hope to get a copy of the film so she can have an opportunity to view it   oh would i love to see it again too  \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WmohTC8Aev1"
      },
      "source": [
        "def _token_StringList(StrList,lemObj):\n",
        "  \n",
        "  wordList,spl_strs  = [],[\"<sos>\",\"<eos>\",\"<pad>\"]\n",
        "  for eLine in StrList:\n",
        "    eLine = eLine.split(\" \")\n",
        "    for eWord in eLine:\n",
        "      if eWord in stop_Words:continue ## Skipping stop words\n",
        "      else:\n",
        "        if  eWord == '':continue\n",
        "        eWord = lemObj.lemmatize(eWord)\n",
        "        wordList.append(eWord)\n",
        "  return wordList\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "wordToken = _token_StringList(Cleaned_Sentences,wl)\n",
        "\n",
        "#wordToken = {ind:word for ind,word in enumerate(spl_strs+wordList)}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVkaZWRSH4Ws"
      },
      "source": [
        "wordDict = Counter(wordToken)\n",
        "print(wordDict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8xQEx00gOzV"
      },
      "source": [
        "def _return_most_recurringVocab(worDict):\n",
        "  spl_strs = [\"<pad>\"]\n",
        "  vList = [x[0] for x in sorted(worDict.items(),key=lambda x:x[1],reverse=True)[:1000]]\n",
        "  return {word:ind for ind,word in enumerate(spl_strs+vList)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnoQU3D2EbT8"
      },
      "source": [
        "# Train & Test Data(Indexed Vocab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqS2LAJdhuoN"
      },
      "source": [
        "trainVocab = _return_most_recurringVocab(wordDict)\n",
        "print(trainVocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ94bwQd6QKb"
      },
      "source": [
        "## Similar activity for Test Data ##\n",
        "test_Cleaned_Sentences =  _string_cleanUp(X_test)\n",
        "testWordToken = _token_StringList(test_Cleaned_Sentences,wl)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ssfkgEGiY8c"
      },
      "source": [
        "testVocab = _return_most_recurringVocab(Counter(testWordToken))\n",
        "print(testVocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LYKeAF7ELBF"
      },
      "source": [
        "# Custom Data Loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoaBbSbJe51b"
      },
      "source": [
        "## Create a custom dataset loader ## \n",
        "class _reviews_loader(Dataset):\n",
        "  def __init__(self,X,Y):\n",
        "    super().__init__()\n",
        "    self.X,self.Y = X,Y\n",
        "    \n",
        "  \n",
        "  def __len__(self):\n",
        "    #d_frame = pd.read_csv(csv_file_name)\n",
        "    return len(self.X)\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    returnDict = (self.X[idx],self.Y[idx])\n",
        "    return returnDict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSeL6X6Yxt6i"
      },
      "source": [
        "class MyCollateClass():\n",
        "  def __init__(self,vocabDict = None):\n",
        "    self.vocabDict = vocabDict\n",
        "\n",
        "  def _string_cleanUp(arrOf_strs):\n",
        "    count=0\n",
        "    listOf_Strs = []\n",
        "    for e_str in arrOf_strs:\n",
        "      e_str = e_str.lower()   ## Loawer Casing the entire string\n",
        "      e_str = re.sub(r'<[^>]*>','',e_str) ## Removing HTML Tags\n",
        "      e_str = re.sub(r\"[^\\w\\s]\", ' ', e_str) ## Remove Special Characters \n",
        "      e_str = re.sub(r\"\\d\", '', e_str) ## Remove Numbers from string\n",
        "      count+=1\n",
        "      listOf_Strs.append(e_str)\n",
        "    return listOf_Strs\n",
        "\n",
        "  def _return_indexList(self,OneSentance):\n",
        "    vocabIndexes = []\n",
        "    for eWord in OneSentance.split(\" \"):\n",
        "      if eWord in list(self.vocabDict.keys()):\n",
        "        vocabIndexes.append(self.vocabDict[eWord])\n",
        "    idx_Tensor = torch.LongTensor(vocabIndexes)\n",
        "    return idx_Tensor\n",
        "  \n",
        "  def _stack_Sentance_info(self,max_sentence_len = None,batch_size=None,device='cpu'):\n",
        "    tensorList,updatedTensorList,seqLengths = [],[],[]\n",
        "    for ind,eLine in enumerate(self.cleanedList):\n",
        "      retTensor = self._return_indexList(eLine)\n",
        "      tensorList.append(retTensor)\n",
        "    maxTensorSize = max(list((e_Tensor.size()[0] for e_Tensor in tensorList)))\n",
        "    for e_tensor in tensorList:\n",
        "      seqLengths.append(e_tensor.size()[0])\n",
        "      if e_tensor.size()[0]<maxTensorSize:\n",
        "        diff = maxTensorSize - e_tensor.size()[0]\n",
        "        newTensor = torch.cat([e_tensor,torch.zeros(diff)])\n",
        "        updatedTensorList.append(newTensor)\n",
        "      else:updatedTensorList.append(e_tensor)\n",
        "    finalTensor = torch.stack(updatedTensorList).type(torch.LongTensor).to(device)\n",
        "    return finalTensor,seqLengths\n",
        "\n",
        "  def PadCollate(self,batch):\n",
        "    def _get_max_sentance_len(SentanceList):\n",
        "      return max(list((len(esentance.split(' ')) for esentance in SentanceList)))\n",
        "    def _convert_senti_to_int(SentList,device='cpu'):\n",
        "      sTensor = torch.LongTensor(SentList)\n",
        "      return sTensor\n",
        "    batch_Dict = {}\n",
        "    revList = list((eTuple[0] for eTuple in batch))\n",
        "    sentiList = list((eTuple[1] for eTuple in batch))\n",
        "    stacked_senti_tensor = _convert_senti_to_int(sentiList,device=Mydevice).to(Mydevice)\n",
        "    self.cleanedList = _string_cleanUp(revList)\n",
        "    maxLen_sentance = _get_max_sentance_len(self.cleanedList)\n",
        "    stacked_vocab_tensor,seqLengths = self._stack_Sentance_info(maxLen_sentance,len(batch),device=Mydevice)\n",
        "    batch_Dict = {\"Vocab\":stacked_vocab_tensor,\"Senti\":stacked_senti_tensor,\"Seqlen\":seqLengths}\n",
        "    return batch_Dict\n",
        "\n",
        "\n",
        "  def __call__(self,batch):\n",
        "    return self.PadCollate(batch)\n",
        "\n",
        "review_dataset = _reviews_loader(X_train,Y_train)\n",
        "dataloader1 = DataLoader(review_dataset,batch_size = 10,shuffle=True, num_workers=0,collate_fn=MyCollateClass(trainVocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2mPHDBeB-Eh"
      },
      "source": [
        "for ind,data in enumerate(dataloader1):\n",
        "  if ind>3:break\n",
        "  print(data[\"Vocab\"].device)\n",
        "  print(data[\"Senti\"])\n",
        "  print(data[\"Vocab\"].shape)\n",
        "  print(data[\"Senti\"].shape)\n",
        "  print(\"seq lenght\",data[\"Seqlen\"])\n",
        "  print('*'*75)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ud-6xBEAk6"
      },
      "source": [
        "MODEL\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxwIlfrvEAFT"
      },
      "source": [
        "class SentiClassify_Model(nn.Module):\n",
        "  def __init__(self,vocabLen,dims,hidden_size,batchSize,numLayers,output_size=2):\n",
        "    super().__init__()\n",
        "    #output_size =  2\n",
        "    self.hidden_size = hidden_size\n",
        "    self.batchSize = batchSize\n",
        "    self.numLayers = numLayers\n",
        "    self.embed = nn.Embedding(vocabLen,dims)\n",
        "    self.lstm_cell = nn.LSTM(input_size=dims,hidden_size=hidden_size,batch_first =True,\n",
        "                             num_layers=self.numLayers,bidirectional=True)\n",
        "    self.lf = nn.Linear(self.hidden_size,output_size)\n",
        "    self.dp_out = nn.Dropout(p=0.3)\n",
        "    self.F = nn.ReLU(inplace=False)\n",
        "    \n",
        "    \n",
        "\n",
        "  def forward(self,input,hidden =None,bsize = None,verbose=False):\n",
        "    embeds = self.embed(input)\n",
        "    output,(hid,cell) = self.lstm_cell(embeds,hidden)\n",
        "    if bsize!=None:\n",
        "      hid = hid.view(self.numLayers*self.num_dirns,bsize, self.hidden_size)\n",
        "    else:\n",
        "      hid = hid.view(self.numLayers*self.num_dirns,self.batchSize, self.hidden_size)\n",
        "    # Get the last hidden state with respect to the layers\n",
        "    hid = hid[-1]\n",
        "    # Get rid of the direction dimension (won't work for bidirectional=True)\n",
        "    hid = self.dp_out(hid.squeeze(0))\n",
        "    linear = self.lf(hid)\n",
        "    return linear\n",
        "\n",
        "    if verbose:\n",
        "      print(\"input shape\",input.shape)\n",
        "      print('embed shape',embeds.shape)\n",
        "      print(\"Rehaped output\",reshaped_out.size())\n",
        "      print(\"After Fully conn layer :\",lin_output.size())\n",
        "\n",
        "  def init_hiddenlayer(self,num_dirns=1,device='cpu'):\n",
        "    self.num_dirns = num_dirns\n",
        "    return (torch.zeros(self.numLayers*num_dirns,self.batchSize,self.hidden_size,device=device),torch.zeros(self.numLayers*num_dirns,self.batchSize ,self.hidden_size,device=device))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlqEkoxQS4D-"
      },
      "source": [
        "def computeAccuracy(target,source):\n",
        "  sf_max_obj = nn.Softmax(dim=1)\n",
        "  sf_max = sf_max_obj(source)\n",
        "  sf_max = torch.argmax(sf_max,dim=1)\n",
        "  fintensor = torch.where(sf_max==1,1,0) ## 1-> positive ,0->Negative\n",
        "  score = accuracy_score(target.tolist(),fintensor.tolist())\n",
        "  return score\n",
        "\n",
        "\n",
        "def infer(dataLoader,net,device,batchSize=None):\n",
        "  net.eval().to(device)\n",
        "  allScores = []\n",
        "  if torch.is_tensor(dataLoader):\n",
        "    source = net(dataLoader,bsize=batchSize)\n",
        "    sf_max_obj = nn.Softmax(dim=1)\n",
        "    sf_max = sf_max_obj(source)\n",
        "    sf_max = torch.argmax(sf_max,dim=1)\n",
        "    fintensor = torch.where(sf_max==1,1,0)\n",
        "    return fintensor\n",
        "    \n",
        "  for ind,data in enumerate(dataLoader):\n",
        "    wordInput,seqLengths,targets = data[\"Vocab\"],data[\"Seqlen\"],data[\"Senti\"]\n",
        "    if wordInput.size()[0] != batchSize:continue\n",
        "    source = net(wordInput)\n",
        "    allScores.append(computeAccuracy(targets,source))\n",
        "  net.train().to(device)\n",
        "  return sum(allScores)/len(allScores)   ## Mean Accuracy for all test batches ##\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tdW9kk1trUG"
      },
      "source": [
        "def _trainLoader(model=None,Train_dataset=None,Test_Loader = None, batchSize =None,vocabList = None,optimFn =None,loss_func=None,epochs=1,device='cpu',lr=0.005):\n",
        "  maxLoss = 10000\n",
        "  dataloader1 = DataLoader(Train_dataset,batch_size = batchSize,shuffle=True, num_workers=0,collate_fn=MyCollateClass(vocabList))\n",
        "\n",
        "  loss_per_epoch,train_accuracy = 0,None\n",
        "  ## Batch Optimization ##\n",
        "  for i in range(epochs):\n",
        "    cummLoss = 0\n",
        "    for ind,data in enumerate(tqdm(dataloader1)):\n",
        "      wordInput,seqLengths,targets = data[\"Vocab\"],data[\"Seqlen\"],data[\"Senti\"]\n",
        "      hidden = modObj.init_hiddenlayer(num_dirns = num_dirns,device=device)\n",
        "      if wordInput.size()[0]!=batchSize:continue\n",
        "      optimFn.zero_grad()\n",
        "      source = modObj(wordInput,hidden)\n",
        "      loss = lossFn(source,targets)\n",
        "      loss.backward()\n",
        "      optimFn.step()\n",
        "      cummLoss+=loss.item()*batchSize ## Cumulative loss per batch\n",
        "\n",
        "\n",
        "    loss_per_epoch = cummLoss/batchSize\n",
        "    train_accuracy = computeAccuracy(targets,source)\n",
        "    if loss_per_epoch<maxLoss:\n",
        "      maxLoss = loss_per_epoch\n",
        "      torch.save({\n",
        "          'epoch': i,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimFn.state_dict(),\n",
        "          'loss': loss_per_epoch,\n",
        "          }, \"model.pt\")\n",
        "    if Test_Loader!=None:\n",
        "      Mean_testAccuracy = infer(testLoader,modObj,Mydevice,batchSize)\n",
        "    print(\"Loss per Epoch : {} , Training Accuracy : {}, Mean Test Accuracy : {}\".format(loss_per_epoch,train_accuracy,Mean_testAccuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjEkKU1tsChL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e42669d-2344-4ea2-f333-3fb5e4f856bb"
      },
      "source": [
        "### Hyperparameters ##\n",
        "embed_dims = 20\n",
        "hidden_size = 30\n",
        "num_LSTMLayers = 2\n",
        "num_dirns = 2\n",
        "batchSize = 1000\n",
        "lr = 0.005\n",
        "#################################################\n",
        "review_dataset = _reviews_loader(X_train,Y_train)\n",
        "test_data =  _reviews_loader(X_test,Y_test)\n",
        "testLoader = DataLoader(test_data,batch_size = batchSize,shuffle=True, num_workers=0,collate_fn=MyCollateClass(testVocab))\n",
        "modObj = SentiClassify_Model(len(trainVocab),embed_dims,hidden_size,batchSize,num_LSTMLayers).to(Mydevice)\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "optimm = optim.Adam(modObj.parameters(),lr=lr)\n",
        "_trainLoader(model=modObj,Train_dataset=review_dataset,Test_Loader = testLoader,batchSize=batchSize, vocabList = trainVocab,loss_func=lossFn,optimFn = optimm, epochs=10,device=Mydevice)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:28<00:00,  4.96s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 19.894886016845703 , Training Accuracy : 0.698, Mean Test Accuracy : 0.56165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:28<00:00,  4.95s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 16.560502350330353 , Training Accuracy : 0.754, Mean Test Accuracy : 0.5803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:29<00:00,  4.98s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 14.02003327012062 , Training Accuracy : 0.799, Mean Test Accuracy : 0.6012000000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:30<00:00,  5.01s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 12.18629252910614 , Training Accuracy : 0.836, Mean Test Accuracy : 0.59285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:29<00:00,  4.98s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 11.10161018371582 , Training Accuracy : 0.844, Mean Test Accuracy : 0.6045999999999998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:29<00:00,  4.97s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 10.306888580322266 , Training Accuracy : 0.853, Mean Test Accuracy : 0.6073000000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:29<00:00,  4.99s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 9.815760970115662 , Training Accuracy : 0.859, Mean Test Accuracy : 0.6043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:30<00:00,  5.00s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 10.048907577991486 , Training Accuracy : 0.862, Mean Test Accuracy : 0.6012000000000002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:29<00:00,  4.98s/it]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 9.276146978139877 , Training Accuracy : 0.889, Mean Test Accuracy : 0.5977999999999999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [02:29<00:00,  4.98s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss per Epoch : 8.978687703609467 , Training Accuracy : 0.88, Mean Test Accuracy : 0.6009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d799xD6KFcw"
      },
      "source": [
        "def _get_max_sentance_len(SentanceList):\n",
        "  return max(list((len(esentance.split(' ')) for esentance in SentanceList)))\n",
        "\n",
        "def _return_indexList(OneSentance,vocabDict):\n",
        "  vocabIndexes = []\n",
        "  for eWord in OneSentance.split(\" \"):\n",
        "    if eWord in list(vocabDict.keys()):\n",
        "      vocabIndexes.append(vocabDict[eWord])\n",
        "  idx_Tensor = torch.LongTensor(vocabIndexes)\n",
        "  return idx_Tensor\n",
        "\n",
        "def _stack_Sentance_info(cleanedList,vocanDict,max_sentence_len = None,batch_size=None,device='cpu'):\n",
        "  tensorList,updatedTensorList,seqLengths = [],[],[]\n",
        "  for ind,eLine in enumerate(cleanedList):\n",
        "    retTensor = _return_indexList(eLine,vocanDict)\n",
        "    tensorList.append(retTensor)\n",
        "  maxTensorSize = max(list((e_Tensor.size()[0] for e_Tensor in tensorList)))\n",
        "  for e_tensor in tensorList:\n",
        "    seqLengths.append(e_tensor.size()[0])\n",
        "    if e_tensor.size()[0]<maxTensorSize:\n",
        "      diff = maxTensorSize - e_tensor.size()[0]\n",
        "      newTensor = torch.cat([e_tensor,torch.zeros(diff)])\n",
        "      updatedTensorList.append(newTensor)\n",
        "    else:updatedTensorList.append(e_tensor)\n",
        "  finalTensor = torch.stack(updatedTensorList).type(torch.LongTensor).to(device)\n",
        "  return finalTensor,seqLengths"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY3VQY_0uEFl"
      },
      "source": [
        "checkpoint = torch.load('model.pt')\n",
        "modObj.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimm.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxM-H1CqHz_q",
        "outputId": "17a2b978-c303-4774-b35f-e60c6ba25ecd"
      },
      "source": [
        "sample_reviews = [\"This movie was just awesome!\",\"It was a good movie\",\"His acting was top class and twas a great  movie !\",\"His tastes is utterly bad\"]\n",
        "cleanList = _string_cleanUp(sample_reviews)\n",
        "maxLen = _get_max_sentance_len(cleanList)\n",
        "wordInputs = _stack_Sentance_info(cleanList,trainVocab,max_sentence_len=maxLen,batch_size=len(cleanList),device=Mydevice)[0]\n",
        "pred_output = infer(wordInputs,modObj,Mydevice,batchSize=len(wordInputs))\n",
        "print(pred_output)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 1, 0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqEYDiiHuvLA",
        "outputId": "b34b80d0-a20d-492d-8eff-2a336bd8a1bc"
      },
      "source": [
        "sample_tens = torch.tensor([[0.45,0.5],\n",
        "                            [0.5,0.48]])\n",
        "print(sample_tens)\n",
        "bb = torch.argmax(sample_tens,dim=1)\n",
        "print(bb)\n",
        "bb = torch.where(bb==1,1,0)\n",
        "print(bb)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4500, 0.5000],\n",
            "        [0.5000, 0.4800]])\n",
            "tensor([1, 0])\n",
            "tensor([1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}